#!/usr/bin/env python3
"""
AI Hub ë„™ì¹˜ ì§ˆë³‘ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import requests
import zipfile
from pathlib import Path
import json
from tqdm import tqdm

class AIHubDatasetDownloader:
    def __init__(self, api_key: str, download_dir: str = "data/aihub_flounder"):
        self.api_key = api_key
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        # AI Hub API ì„¤ì •
        self.base_url = "https://api.aihub.or.kr"
        self.dataset_id = "71345"  # ë„™ì¹˜ ì§ˆë³‘ ë°ì´í„°ì…‹ ID
        
    def download_dataset(self):
        """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        print("ğŸŸ AI Hub ë„™ì¹˜ ì§ˆë³‘ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        # API ì¸ì¦
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        try:
            # ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ
            dataset_info = self._get_dataset_info(headers)
            print(f"ğŸ“‹ ë°ì´í„°ì…‹ ì •ë³´: {dataset_info.get('name', 'Unknown')}")
            
            # íŒŒì¼ ëª©ë¡ ì¡°íšŒ
            file_list = self._get_file_list(headers)
            print(f"ğŸ“ ì´ {len(file_list)} ê°œ íŒŒì¼ ë°œê²¬")
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            for file_info in tqdm(file_list, desc="ë‹¤ìš´ë¡œë“œ ì§„í–‰"):
                self._download_file(file_info, headers)
                
            print("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”.")
            self._manual_download_guide()
    
    def _get_dataset_info(self, headers):
        """ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ"""
        url = f"{self.base_url}/datasets/{self.dataset_id}"
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    
    def _get_file_list(self, headers):
        """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        url = f"{self.base_url}/datasets/{self.dataset_id}/files"
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    
    def _download_file(self, file_info, headers):
        """ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        file_id = file_info['id']
        filename = file_info['name']
        
        url = f"{self.base_url}/datasets/{self.dataset_id}/files/{file_id}/download"
        
        response = requests.get(url, headers=headers, stream=True)
        response.raise_for_status()
        
        file_path = self.download_dir / filename
        
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=1024*1024):  # 1MB chunks
                if chunk:
                    f.write(chunk)
    
    def _manual_download_guide(self):
        """ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ"""
        print("\nğŸ“– ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
        print("1. https://www.aihub.or.kr ë¡œê·¸ì¸")
        print("2. ë„™ì¹˜ ì§ˆë³‘ ë°ì´í„°ì…‹ í˜ì´ì§€ ì ‘ì†")
        print("3. 'ë°ì´í„° ë‹¤ìš´ë¡œë“œ' ë²„íŠ¼ í´ë¦­")
        print("4. ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ data/aihub_flounder/ í´ë”ì— ì••ì¶• í•´ì œ")
        print("\nğŸ“ ì˜ˆìƒ í´ë” êµ¬ì¡°:")
        print("data/aihub_flounder/")
        print("â”œâ”€â”€ Training/")
        print("â”‚   â”œâ”€â”€ RGB/")
        print("â”‚   â””â”€â”€ Hyperspectral/")
        print("â”œâ”€â”€ Validation/")
        print("â””â”€â”€ annotations.json")
    
    def extract_and_organize(self):
        """ë‹¤ìš´ë¡œë“œí•œ ì••ì¶• íŒŒì¼ ì¶”ì¶œ ë° ì •ë¦¬"""
        print("ğŸ“¦ ì••ì¶• íŒŒì¼ ì¶”ì¶œ ì¤‘...")
        
        zip_files = list(self.download_dir.glob("*.zip"))
        
        for zip_file in zip_files:
            print(f"ğŸ“‚ {zip_file.name} ì¶”ì¶œ ì¤‘...")
            
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                zip_ref.extractall(self.download_dir)
            
            # ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            # zip_file.unlink()
        
        print("âœ… ì¶”ì¶œ ì™„ë£Œ!")
        self._analyze_dataset_structure()
    
    def _analyze_dataset_structure(self):
        """ë°ì´í„°ì…‹ êµ¬ì¡° ë¶„ì„"""
        print("\nğŸ” ë°ì´í„°ì…‹ êµ¬ì¡° ë¶„ì„:")
        
        total_images = 0
        disease_types = set()
        
        for root, dirs, files in os.walk(self.download_dir):
            image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            if image_files:
                print(f"ğŸ“ {root}: {len(image_files)} ì´ë¯¸ì§€")
                total_images += len(image_files)
        
        print(f"\nğŸ“Š ì´ ì´ë¯¸ì§€ ìˆ˜: {total_images}")
        
        # ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ë¶„ì„
        annotation_files = list(self.download_dir.glob("**/annotations.json"))
        if annotation_files:
            with open(annotation_files[0], 'r', encoding='utf-8') as f:
                annotations = json.load(f)
                print(f"ğŸ“‹ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°: {len(annotations)} ê°œ")


def main():
    # ì‚¬ìš©ë²•
    print("ğŸš€ AI Hub ë„™ì¹˜ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë”")
    print("=" * 50)
    
    # API í‚¤ ì…ë ¥ (AI Hubì—ì„œ ë°œê¸‰)
    api_key = input("AI Hub API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—†ìœ¼ë©´ Enter): ").strip()
    
    downloader = AIHubDatasetDownloader(api_key)
    
    if api_key:
        # APIë¥¼ í†µí•œ ìë™ ë‹¤ìš´ë¡œë“œ
        downloader.download_dataset()
    else:
        # ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ
        downloader._manual_download_guide()
    
    # ì••ì¶• í•´ì œ ì—¬ë¶€ í™•ì¸
    if input("\nì••ì¶• íŒŒì¼ì„ ì¶”ì¶œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y':
        downloader.extract_and_organize()


if __name__ == "__main__":
    main()