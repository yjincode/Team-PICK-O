#!/usr/bin/env python3
"""
ë„™ì¹˜ ì§ˆë³‘ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë¶„ì„
"""

import os
import json
import cv2
import numpy as np
from pathlib import Path
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import pandas as pd

class FlounderDatasetProcessor:
    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)
        self.annotations = {}
        self.disease_mapping = {
            # AI Hub ë°ì´í„°ì…‹ì˜ ì§ˆë³‘ ë¼ë²¨ì„ ìš°ë¦¬ ì‹œìŠ¤í…œì— ë§ê²Œ ë§¤í•‘
            'bacterial_infection': ['ì„¸ê· ì„±ê°ì—¼', 'ì„¸ê· ê°ì—¼', 'bacterial'],
            'viral_infection': ['ë°”ì´ëŸ¬ìŠ¤ê°ì—¼', 'viral'],  
            'parasitic_infection': ['ê¸°ìƒì¶©ê°ì—¼', 'parasite'],
            'skin_lesion': ['í”¼ë¶€ë³‘ë³€', 'ìƒì²˜', 'lesion'],
            'healthy': ['ì •ìƒ', 'ê±´ê°•', 'normal', 'healthy']
        }
        
    def load_annotations(self):
        """ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ë¡œë“œ"""
        print("ğŸ“‹ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        annotation_files = list(self.dataset_path.glob("**/annotations.json"))
        if not annotation_files:
            annotation_files = list(self.dataset_path.glob("**/*.json"))
            
        for ann_file in annotation_files:
            try:
                with open(ann_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.annotations.update(data)
                    print(f"âœ… {ann_file.name}: {len(data)} ê°œ ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ")
            except Exception as e:
                print(f"âŒ {ann_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ“Š ì´ ì–´ë…¸í…Œì´ì…˜: {len(self.annotations)} ê°œ")
    
    def analyze_dataset(self):
        """ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„"""
        print("\nğŸ” ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘...")
        
        disease_counts = Counter()
        image_sizes = []
        severity_counts = Counter()
        
        for img_id, annotation in self.annotations.items():
            # ì§ˆë³‘ íƒ€ì… ë¶„ì„
            disease_type = annotation.get('disease_type', 'unknown')
            disease_counts[disease_type] += 1
            
            # ì‹¬ê°ë„ ë¶„ì„
            severity = annotation.get('severity', 'unknown')
            severity_counts[severity] += 1
            
            # ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„
            if 'width' in annotation and 'height' in annotation:
                image_sizes.append((annotation['width'], annotation['height']))
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ì§ˆë³‘ íƒ€ì… ë¶„í¬:")
        for disease, count in disease_counts.most_common():
            print(f"  {disease}: {count} ê°œ ({count/len(self.annotations)*100:.1f}%)")
        
        print("\nğŸ“Š ì‹¬ê°ë„ ë¶„í¬:")
        for severity, count in severity_counts.most_common():
            print(f"  {severity}: {count} ê°œ")
        
        if image_sizes:
            avg_width = np.mean([size[0] for size in image_sizes])
            avg_height = np.mean([size[1] for size in image_sizes])
            print(f"\nğŸ“ í‰ê·  ì´ë¯¸ì§€ í¬ê¸°: {avg_width:.0f} x {avg_height:.0f}")
        
        return disease_counts, severity_counts
    
    def extract_disease_regions(self, output_dir: str = "data/disease_regions"):
        """ì§ˆë³‘ ë¶€ìœ„ë³„ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        print("\nâœ‚ï¸  ì§ˆë³‘ ë¶€ìœ„ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘...")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ì§ˆë³‘ íƒ€ì…ë³„ í´ë” ìƒì„±
        for disease_type in self.disease_mapping.keys():
            (output_path / disease_type).mkdir(exist_ok=True)
        
        extracted_count = 0
        
        for img_id, annotation in self.annotations.items():
            try:
                # ì›ë³¸ ì´ë¯¸ì§€ ì°¾ê¸°
                img_path = self._find_image_path(img_id)
                if not img_path:
                    continue
                
                # ì´ë¯¸ì§€ ë¡œë“œ
                image = cv2.imread(str(img_path))
                if image is None:
                    continue
                
                # ì§ˆë³‘ ë¶€ìœ„ ì¶”ì¶œ
                disease_type = self._map_disease_type(annotation.get('disease_type', 'unknown'))
                
                if 'bbox' in annotation:
                    # ë°”ìš´ë”© ë°•ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
                    bbox = annotation['bbox']
                    x, y, w, h = bbox['x'], bbox['y'], bbox['width'], bbox['height']
                    
                    # ROI ì¶”ì¶œ
                    roi = image[y:y+h, x:x+w]
                    
                    # ì €ì¥
                    save_path = output_path / disease_type / f"{img_id}_{extracted_count}.jpg"
                    cv2.imwrite(str(save_path), roi)
                    extracted_count += 1
                else:
                    # ì „ì²´ ì´ë¯¸ì§€ ì‚¬ìš©
                    save_path = output_path / disease_type / f"{img_id}_{extracted_count}.jpg"
                    cv2.imwrite(str(save_path), image)
                    extracted_count += 1
                    
            except Exception as e:
                print(f"âš ï¸  {img_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"âœ… {extracted_count} ê°œ ì§ˆë³‘ ë¶€ìœ„ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ")
    
    def _find_image_path(self, img_id: str) -> Path:
        """ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°"""
        possible_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        
        for root in [self.dataset_path / "Training", self.dataset_path / "Validation", self.dataset_path]:
            for ext in possible_extensions:
                img_path = root / f"{img_id}{ext}"
                if img_path.exists():
                    return img_path
                    
                # RGB í´ë” ë‚´ ê²€ìƒ‰
                rgb_path = root / "RGB" / f"{img_id}{ext}"
                if rgb_path.exists():
                    return rgb_path
        
        return None
    
    def _map_disease_type(self, original_type: str) -> str:
        """ì›ë³¸ ì§ˆë³‘ íƒ€ì…ì„ ìš°ë¦¬ ì‹œìŠ¤í…œ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘"""
        original_lower = original_type.lower()
        
        for our_type, original_types in self.disease_mapping.items():
            for orig in original_types:
                if orig.lower() in original_lower:
                    return our_type
        
        return 'unknown'
    
    def create_training_dataset(self, output_dir: str = "data/training_data"):
        """í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„±"""
        print("\nğŸ¯ í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        
        output_path = Path(output_dir)
        
        # train/val í´ë” ìƒì„±
        for split in ['train', 'val']:
            for disease_type in self.disease_mapping.keys():
                (output_path / split / disease_type).mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ë¶„í•  (80% train, 20% val)
        disease_images = defaultdict(list)
        
        for img_id, annotation in self.annotations.items():
            disease_type = self._map_disease_type(annotation.get('disease_type', 'unknown'))
            disease_images[disease_type].append((img_id, annotation))
        
        # ê° ì§ˆë³‘ íƒ€ì…ë³„ë¡œ ë¶„í• 
        for disease_type, images in disease_images.items():
            np.random.shuffle(images)
            
            split_idx = int(len(images) * 0.8)
            train_images = images[:split_idx]
            val_images = images[split_idx:]
            
            # í›ˆë ¨ ë°ì´í„° ë³µì‚¬
            for img_id, annotation in train_images:
                self._copy_image_to_split(img_id, disease_type, 'train', output_path)
            
            # ê²€ì¦ ë°ì´í„° ë³µì‚¬  
            for img_id, annotation in val_images:
                self._copy_image_to_split(img_id, disease_type, 'val', output_path)
            
            print(f"  {disease_type}: train={len(train_images)}, val={len(val_images)}")
        
        print("âœ… í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
    
    def _copy_image_to_split(self, img_id: str, disease_type: str, split: str, output_path: Path):
        """ì´ë¯¸ì§€ë¥¼ train/val í´ë”ë¡œ ë³µì‚¬"""
        src_path = self._find_image_path(img_id)
        if src_path:
            dst_path = output_path / split / disease_type / f"{img_id}.jpg"
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì €ì¥ (í˜•ì‹ í†µì¼)
            image = cv2.imread(str(src_path))
            if image is not None:
                cv2.imwrite(str(dst_path), image)


def main():
    print("ğŸŸ ë„™ì¹˜ ì§ˆë³‘ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ê¸°")
    print("=" * 50)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ ì…ë ¥
    dataset_path = input("ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (data/aihub_flounder): ").strip()
    if not dataset_path:
        dataset_path = "data/aihub_flounder"
    
    if not Path(dataset_path).exists():
        print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_path}")
        print("ğŸ’¡ ë¨¼ì € download_aihub_dataset.pyë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
        return
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = FlounderDatasetProcessor(dataset_path)
    
    # ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ
    processor.load_annotations()
    
    if not processor.annotations:
        print("âŒ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ì…‹ ë¶„ì„
    processor.analyze_dataset()
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\nğŸ“‹ ìˆ˜í–‰í•  ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì§ˆë³‘ ë¶€ìœ„ ì´ë¯¸ì§€ ì¶”ì¶œ")
    print("2. í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„±")
    print("3. ëª¨ë‘ ìˆ˜í–‰")
    
    choice = input("ì„ íƒ (1-3): ").strip()
    
    if choice in ['1', '3']:
        processor.extract_disease_regions()
    
    if choice in ['2', '3']:
        processor.create_training_dataset()
    
    print("\nğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì§ˆë³‘ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨")


if __name__ == "__main__":
    main()